# Airlift Development Rules and Guidelines

## Project Context

Airlift is a Python command-line tool for uploading CSV/JSON data with attachments to Airtable. The project uses a modular architecture with clear separation of concerns across data processing, API integration, and file handling components.

## Quick Reference Commands

### Building and Testing
```bash
# Build the project using the ephemeral build system (recommended)
./scripts/local-test-build.sh

# Test the built binary
./test-build/airlift --help

# Clean build environment
./scripts/local-test-build.sh --clean

# Build with dependency updates
./scripts/local-test-build.sh --update-deps

# Run comprehensive tests (no API tokens required)
./scripts/local-test-build.sh --comprehensive-test

# Install dependencies for development
poetry install

# Run the application in development
poetry run airlift --help
```

### Dependency Management
```bash
# Update poetry.lock file
poetry lock

# Update specific packages
./scripts/local-test-build.sh --update requests dropbox

# Show outdated packages
./scripts/local-test-build.sh --show-outdated

# Install new dependency
poetry add <package-name>

# Install development dependency
poetry add --group dev <package-name>
```

## Code Organization

### Module Structure
- Keep each module focused on a single responsibility
- Use clear, descriptive module names that reflect their purpose
- Maintain consistent import patterns across modules
- Avoid circular dependencies between modules

### File Naming Conventions
- Use snake_case for all Python files and functions
- Use descriptive names that clearly indicate functionality
- Follow the existing naming pattern: `airlift_*.py` for core modules

## Coding Standards

### Python Style
- Follow PEP 8 style guidelines strictly
- Use type hints for all function parameters and return values
- Implement comprehensive docstrings for public functions and classes
- Use logging instead of print statements for all output
- Keep line length under 88 characters (Black formatter standard)

### Error Handling
- Use custom exception classes from `utils_exceptions.py`
- Implement proper exception chaining with `raise ... from`
- Provide meaningful error messages to end users
- Log detailed error information for debugging
- Handle both critical and non-critical errors appropriately

### Data Processing
- Validate input data before processing
- Use UTF-8 encoding for all file operations
- Handle missing or malformed data gracefully
- Implement proper data type conversion and validation

## API Integration Patterns

### Airtable API
- Use the `new_client` class for all Airtable operations
- Implement proper authentication with Bearer tokens
- Handle API rate limits and errors gracefully
- Use structured JSON payloads for data uploads
- Validate responses and handle error codes appropriately
- Use pyairtable 3.x APIs for field creation and schema management
- Use batch operations for delete (10 records per API call)

### Dropbox API
- Use the `dropbox_client` class for file operations
- Implement OAuth2 flow for authentication with explicit scopes
- Handle refresh token management properly
- Create organized folder structures for uploads
- Generate proper sharing URLs for attachments
- Use Dropbox SDK 12.x for latest API features and security

## CLI Development

### Argument Parsing
- Use the existing `cli_args.py` structure for argument definitions
- Group related arguments logically (general, dropbox, column, validation, database options)
- Provide clear help text for all options
- Implement proper validation for required arguments in `_validate_required_args()`
- Use appropriate data types for argument values

### CLI Arguments Structure
```python
schema: ArgSchema = {
    "POSITIONAL": { ... },
    "general_options": { ... },
    "dropbox options": { ... },
    "column_options": { ... },
    "custom application options": { ... },
    "validation_options": { ... },
    "database_options": { ... },  # --delete-all-database-entries, --empty-dropbox-folder
}
```

### User Experience
- Provide clear progress indicators for long operations (tqdm)
- Use consistent logging levels (INFO, WARNING, ERROR, DEBUG)
- Implement verbose mode for detailed debugging output
- Handle user interruptions gracefully (Ctrl+C)
- Show helpful error messages for missing required arguments

## Performance Considerations

### Concurrency
- Use ThreadPoolExecutor for parallel upload operations
- Implement configurable worker thread counts (default: 5)
- Avoid blocking operations in worker threads
- Use proper queue management for data distribution

### Memory Management
- Process data in chunks for large files
- Avoid loading entire datasets into memory
- Use generators where appropriate for data iteration
- Implement proper cleanup of resources

## Testing Guidelines

### Test Structure
```
tests/
├── __init__.py                      # Test suite module
├── input_command.py                 # Args configuration and environment variables
├── test_upload.py                   # Upload tests (requires API tokens)
├── test_delete_database_entries.py  # Delete tests (requires API tokens)
├── test_empty_dropbox_folder.py     # Dropbox folder tests (requires Dropbox tokens)
├── test_comprehensive.py            # Comprehensive tests (no API tokens required)
├── README.md                        # Test documentation
└── assets/                          # Test data files
```

### Unit Testing
- Write tests for all public functions and classes
- Mock external API calls to avoid network dependencies
- Test both success and error conditions
- Use descriptive test names that explain the scenario
- Add tests for new CLI arguments in `test_comprehensive.py`

### Integration Testing
- Test end-to-end workflows with sample data
- Verify proper error handling and recovery
- Test with different file formats and sizes
- Validate API integration points

### Running Tests
```bash
# Comprehensive tests (no API tokens required)
./scripts/local-test-build.sh --comprehensive-test

# Or directly with pytest
pytest tests/test_comprehensive.py -v

# Integration tests (requires API tokens)
pytest tests/test_upload.py -v -s
pytest tests/test_delete_database_entries.py -v -s
pytest tests/test_empty_dropbox_folder.py -v -s
```

## Security Best Practices

### Authentication
- Never hardcode credentials in source code
- Use secure token storage mechanisms
- Implement proper OAuth2 flows for external services with explicit scopes
- Validate all user inputs and file paths

### Data Handling
- Sanitize all user inputs and file paths
- Use secure file upload mechanisms
- Implement proper error message sanitization
- Handle sensitive data appropriately

## Documentation Standards

### Code Documentation
- Write comprehensive docstrings for all public functions
- Include parameter types, return types, and examples
- Document any complex algorithms or business logic
- Keep documentation up to date with code changes

### User Documentation
- Maintain clear README with usage examples
- Document all command-line options and their effects
- Provide troubleshooting guides for common issues
- Keep changelog updated with all releases

## Dependencies and Packaging

### Dependency Management
- Use Poetry 2.1.3 for dependency management
- Keep dependencies up to date and secure
- Minimize external dependencies where possible
- Document any version-specific requirements
- Use identical versions to GitHub Actions workflow for reproducible builds
- Maintain Dropbox SDK 12.x for API compatibility and security

### Core Dependencies
| Package | Version | Description |
|---------|---------|-------------|
| `pyairtable` | ^3.3.0 | Airtable API client with latest 3.x APIs |
| `dropbox` | ^12.0.2 | Dropbox API client with OAuth2 scopes |
| `requests` | ^2.32.4 | HTTP client for API calls |
| `tqdm` | ^4.66.4 | Progress bar implementation |
| `pydantic` | ^2.11.7 | Data validation and serialization |
| `icecream` | ^2.1.3 | Debug logging utility |

### Build and Distribution
- Maintain cross-platform compatibility
- Use PyInstaller for binary distribution
- Implement proper versioning with semantic versioning
- Test builds on all target platforms

## Ephemeral Build System

### Local Test Build Script

The project includes a comprehensive ephemeral build system (`scripts/local-test-build.sh`) that must be used for all development and testing:

#### Build System Requirements
- Always use the ephemeral build system for development
- Never install dependencies globally or modify system Python
- Keep PyInstaller out of pyproject.toml (install separately during build)
- Maintain consistency with GitHub Actions workflow
- Use system Python 3.9+ with virtual environments for isolation
- Use identical versions to GitHub Actions workflow for reproducible builds

#### Build Process Standards
- Create virtual environment in `.build/python/` directory using system Python 3.9+
- Install setuptools 80.9.0 in the virtual environment (matches GitHub Actions)
- Install Poetry 2.1.3 in the virtual environment using pip
- Install project dependencies via Poetry in `.build/venv/`
- Install PyInstaller separately in Poetry virtual environment
- Output build artifacts to `test-build/` directory
- Clean up build environment when needed

#### Directory Structure Compliance
```
Airlift/
├── .build/                    # Ephemeral build environment (gitignored)
├── test-build/               # Build output (gitignored)
├── scripts/
│   ├── local-test-build.sh  # Build script
│   └── README.md            # Usage documentation
├── tests/
│   ├── test_comprehensive.py        # Comprehensive tests (no API tokens)
│   ├── test_upload.py               # Upload tests (requires API tokens)
│   ├── test_delete_database_entries.py  # Delete tests (requires API tokens)
│   └── test_empty_dropbox_folder.py     # Dropbox folder tests (requires Dropbox tokens)
└── ... (other project files)
```

#### Build Script Usage
```bash
# Basic build
./scripts/local-test-build.sh

# Clean build environment
./scripts/local-test-build.sh --clean

# Build with dependency updates
./scripts/local-test-build.sh --update-deps

# Update specific packages
./scripts/local-test-build.sh --update requests pytest

# Show outdated packages
./scripts/local-test-build.sh --show-outdated

# Run comprehensive tests
./scripts/local-test-build.sh --comprehensive-test

# Test built binary
./test-build/airlift --help
```

#### CI/CD Integration Standards
- Build script must work identically in local and CI environments
- Use identical versions to GitHub Actions workflow:
  - Python 3.9+ (matches BUILD_PYTHON_VERSION: 3.9)
  - Poetry 2.1.3 (matches BUILD_POETRY_VERSION: 2.1.3)
  - Setuptools 80.9.0 (matches setuptools==80.9.0)
  - PyInstaller latest version (matches CI workflow)
  - poetry-plugin-export for requirements.txt generation
- No system-level installations or modifications
- PyInstaller installed separately (not in pyproject.toml)
- Completely ephemeral and safe for any environment
- Consistent with production build process

## GitHub Actions Workflows

### Workflow Files
| Workflow | Trigger | Description |
|----------|---------|-------------|
| `unit_tests.yml` | Push/PR to master/dev | Comprehensive tests (no API tokens) |
| `airtable_image_upload_test.yml` | Weekly/Manual | Upload integration test |
| `airtable_delete_database_entries_test.yml` | Weekly/Manual | Delete entries + Empty Dropbox folder tests |
| `build.yml` | Push/PR | Build verification |
| `release_github.yml` | Release | Binary distribution |

## Error Handling Patterns

### Exception Hierarchy
- Use `CriticalError` for fatal application errors
- Use `AirtableError` for Airtable-specific issues
- Use `TypeConversionError` for data type issues
- Implement proper exception chaining

### Logging Strategy
- Use structured logging with appropriate levels
- Include context information in log messages
- Implement file-based logging for debugging
- Use consistent log message formatting

## Code Review Guidelines

### Review Checklist
- Verify proper error handling implementation
- Check for security vulnerabilities
- Ensure proper logging and debugging support
- Validate performance implications
- Confirm documentation updates
- Verify build script compatibility
- Add tests for new features

### Quality Standards
- Maintain high test coverage for new features
- Ensure backward compatibility when possible
- Follow established naming conventions
- Implement proper type hints throughout
- Test builds using the ephemeral build system
- Ensure version consistency between local and CI environments

## Maintenance and Support

### Version Management
- Use semantic versioning for releases
- Maintain backward compatibility when possible
- Document breaking changes clearly
- Keep dependencies updated and secure
- Monitor Dropbox SDK releases for security updates and new features

### Monitoring and Debugging
- Implement comprehensive logging for troubleshooting
- Use performance timing utilities for optimization
- Provide clear error messages for users
- Maintain debugging capabilities in production builds

## Integration Guidelines

### External Services
- Implement proper retry logic for API calls
- Handle service outages gracefully
- Use appropriate timeouts for network operations
- Implement proper authentication flows

### User Experience
- Provide clear feedback for all operations
- Implement progress tracking for long operations (tqdm)
- Use intuitive error messages
- Support both interactive and non-interactive modes

## Adding New Features Checklist

When adding new CLI features:
1. Add argument definition in `cli_args.py` schema
2. Handle the argument in `cli.py`
3. Add validation in `_validate_required_args()` if required
4. Implement the feature in appropriate module
5. Add tests in `test_comprehensive.py`
6. Update `AGENT.MD` documentation
7. Update `README.md` if user-facing
8. Run comprehensive tests: `./scripts/local-test-build.sh --comprehensive-test`
9. Build and test binary: `./scripts/local-test-build.sh`

---

This cursorrule file should be kept in sync with the AGENT.MD file to ensure consistent development practices and maintain high code quality standards across the Airlift project.
